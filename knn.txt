>import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns>

     

>dataset=pd.read_csv('/content/sample_data/sonar data.csv')>

     


>dataset>
     
0.0200	0.0371	0.0428	0.0207	0.0954	0.0986	0.1539	0.1601	0.3109	0.2111	...	0.0027	0.0065	0.0159	0.0072	0.0167	0.0180	0.0084	0.0090	0.0032	R
0	0.0453	0.0523	0.0843	0.0689	0.1183	0.2583	0.2156	0.3481	0.3337	0.2872	...	0.0084	0.0089	0.0048	0.0094	0.0191	0.0140	0.0049	0.0052	0.0044	R
1	0.0262	0.0582	0.1099	0.1083	0.0974	0.2280	0.2431	0.3771	0.5598	0.6194	...	0.0232	0.0166	0.0095	0.0180	0.0244	0.0316	0.0164	0.0095	0.0078	R
2	0.0100	0.0171	0.0623	0.0205	0.0205	0.0368	0.1098	0.1276	0.0598	0.1264	...	0.0121	0.0036	0.0150	0.0085	0.0073	0.0050	0.0044	0.0040	0.0117	R
3	0.0762	0.0666	0.0481	0.0394	0.0590	0.0649	0.1209	0.2467	0.3564	0.4459	...	0.0031	0.0054	0.0105	0.0110	0.0015	0.0072	0.0048	0.0107	0.0094	R

>from sklearn.preprocessing import StandardScaler>
     

>scaler=StandardScaler()>
     

>scaler.fit(dataset.drop('R',axis=1))>
     
>StandardScaler()>

>dataset.columns[:-1]>
     
Index(['0.0200', '0.0371', '0.0428', '0.0207', '0.0954', '0.0986', '0.1539',
       '0.1601', '0.3109', '0.2111', '0.1609', '0.1582', '0.2238', '0.0645',
       '0.0660', '0.2273', '0.3100', '0.2999', '0.5078', '0.4797', '0.5783',
       '0.5071', '0.4328', '0.5550', '0.6711', '0.6415', '0.7104', '0.8080',
       '0.6791', '0.3857', '0.1307', '0.2604', '0.5121', '0.7547', '0.8537',
       '0.8507', '0.6692', '0.6097', '0.4943', '0.2744', '0.0510', '0.2834',
       '0.2825', '0.4256', '0.2641', '0.1386', '0.1051', '0.1343', '0.0383',
       '0.0324', '0.0232', '0.0027', '0.0065', '0.0159', '0.0072', '0.0167',
       '0.0180', '0.0084', '0.0090', '0.0032'],
      dtype='object')

>scaled_features=scaler.transform(dataset.drop('R',axis=1))>
     

>scaled_features>
array([[ 0.70018948,  0.42042142,  1.0529498 , ..., -0.4709383 ,
        -0.44268846, -0.42246083],
       [-0.13089402,  0.59942737,  1.71912994, ...,  1.30656071,
         0.25299833,  0.25405324],
       [-0.83579208, -0.64754631,  0.48045125, ..., -0.54822087,
        -0.63683361,  1.03005467],
       ...,
       [ 1.00042384,  0.15949749, -0.67235266, ...,  0.90469137,
        -0.0382194 , -0.68112798],
       [ 0.0475061 , -0.09535845,  0.13434985, ..., -0.00724291,
        -0.70154866, -0.34287094],
       [-0.13959647, -0.06501846, -0.78685237, ..., -0.67187297,
        -0.2970796 ,  0.99025973]])

>new_dataset=pd.DataFrame(scaled_features,columns=dataset.columns[:-1])>
                                              

     

>new_dataset>
     
0.0200	0.0371	0.0428	0.0207	0.0954	0.0986	0.1539	0.1601	0.3109	0.2111	...	0.0232	0.0027	0.0065	0.0159	0.0072	0.0167	0.0180	0.0084	0.0090	0.0032
0	0.700189	0.420421	1.052950	0.319501	0.777810	2.600518	1.522475	2.506911	1.324632	0.587392	...	-0.294569	-0.528057	-0.259343	-0.838780	0.014044	1.913786	1.084838	-0.470938	-0.442688	-0.422461
1	-0.130894	0.599427	1.719130	1.167351	0.401466	2.087862	1.967839	2.847551	3.240336	3.058831	...	-1.061350	1.012809	0.832198	-0.194293	1.227682	2.842696	4.150049	1.306561	0.252998	0.254053
2	-0.835792	-0.647546	0.480451	-0.722021	-0.983262	-1.147115	-0.190961	-0.083126	-0.996071	-0.608898	...	0.672242	-0.142840	-1.010664	0.559893	-0.112965	-0.154353	-0.482599	-0.548221	-0.636834	1.030055
3	2.044717	0.854283	0.110929	-0.315311	-0.289998	-0.671681	-0.011196	1.315846	1.516965	1.768058	...	-0.036197	-1.079853	-0.755498	-0.057168	0.239837	-1.170895	-0.099448	-0.486395	0.447143	0.572413
4	-0.026465	0.208041	-0.419933	-0.788730	-0.660939	-0.094732	-0.024152	0.571138	0.280782	0.711633	...	-0.469595	-0.934096	-1.322533	-0.975905	-1.129034	0.126073
>from sklearn.model_selection import train_test_split
X=new_dataset
y=dataset['R']>
     

>X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=42)>
     

>from sklearn.neighbors import KNeighborsClassifier>
     

>knn=KNeighborsClassifier(n_neighbors=1)>
     

>knn.fit(X_train,y_train)>
     
>KNeighborsClassifier(n_neighbors=1)>

>pred=knn.predict(X_test)>
     

>from sklearn.metrics import classification_report,confusion_matrix>
     

>print(confusion_matrix(y_test,pred))
print(classification_report(y_test,pred))>
[[30  6]
 [ 5 22]]
              precision    recall  f1-score   support

           M       0.86      0.83      0.85        36
           R       0.79      0.81      0.80        27

    accuracy                           0.83        63
   macro avg       0.82      0.82      0.82        63
weighted avg       0.83      0.83      0.83        63


#Analyzing better k value through iterations
>error_rate=[]

for i in range(1,40):
  knn=KNeighborsClassifier(n_neighbors=i)
  knn.fit(X_train,y_train)
  pred_i=knn.predict(X_test)
  error_rate.append(np.mean(pred_i !=y_test))>
     

>plt.figure(figsize=(10,6))
plt.plot(range(1,40),error_rate,color='blue',linestyle='dashed',
         marker='s',markerfacecolor='orange',markersize=10)
plt.title('Error rate vs K values')
plt.xlabel('K')
plt.ylabel('Error rate')
     
Text(0, 0.5, 'Error rate')>
.............
graph
.............
>knn=KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train,y_train)
pred=knn.predict(X_test)
print(confusion_matrix(y_test,pred))
print(classification_report(y_test,pred))>



     
[[33  2]
 [ 2 26]]
              precision    recall  f1-score   support

        Mine       0.94      0.94      0.94        35
        Rock       0.93      0.93      0.93        28

    accuracy                           0.94        63
   macro avg       0.94      0.94      0.94        63
weighted avg       0.94      0.94      0.94        63