>import nltk #since the problem is text classification
>import pandas as pd
     

>nltk.download_shell()
     
NLTK Downloader
---------------------------------------------------------------------------
    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit
---------------------------------------------------------------------------
Downloader> l

Packages:
  [ ] abc................. Australian Broadcasting Commission 2006
  [ ] alpino.............. Alpino Dutch Treebank
  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger
  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)
  [ ] basque_grammars..... Grammars for Basque
  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information
                           Extraction Systems in Biology)
  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model
  [ ] book_grammars....... Grammars from NLTK Book
  [ ] brown............... Brown Corpus
  [ ] brown_tei........... Brown Corpus (TEI XML Version)
  [ ] cess_cat............ CESS-CAT Treebank
  [ ] cess_esp............ CESS-ESP Treebank
  [ ] chat80.............. Chat-80 Data Files
  [ ] city_database....... City Database
  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)
  [ ] comparative_sentences Comparative Sentence Dataset
  [ ] comtrans............ ComTrans Corpus Sample
  [ ] conll2000........... CONLL 2000 Chunking Corpus
  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus
Hit Enter to continue: d
  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan
                           and Basque Subset)
  [ ] crubadan............ Crubadan Corpus
  [ ] dependency_treebank. Dependency Parsed Treebank
  [ ] dolch............... Dolch Word List
  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel
                           Corpus
  [ ] extended_omw........ Extended Open Multilingual WordNet
  [ ] floresta............ Portuguese Treebank
  [ ] framenet_v15........ FrameNet 1.5
  [ ] framenet_v17........ FrameNet 1.7
  [ ] gazetteers.......... Gazeteer Lists
  [ ] genesis............. Genesis Corpus
  [ ] gutenberg........... Project Gutenberg Selections
  [ ] ieer................ NIST IE-ER DATA SAMPLE
  [ ] inaugural........... C-Span Inaugural Address Corpus
  [ ] indian.............. Indian Language POS-Tagged Corpus
  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in
                           ChaSen format)
  [ ] kimmo............... PC-KIMMO Data Files
  [ ] knbc................ KNB Corpus (Annotated blog corpus)
Hit Enter to continue: 
  [ ] large_grammars...... Large context-free and feature-based grammars
                           for parser comparison
  [ ] lin_thesaurus....... Lin's Dependency Thesaurus
  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with
                           part-of-speech tags
  [ ] machado............. Machado de Assis -- Obra Completa
  [ ] masc_tagged......... MASC Tagged Corpus
  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)
  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)
  [ ] moses_sample........ Moses Sample Models
  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0
  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0
  [ ] mwa_ppdb............ The monolingual word aligner (Sultan et al.
                           2015) subset of the Paraphrase Database.
  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)
  [ ] nombank.1.0......... NomBank Corpus 1.0
  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)
  [ ] nps_chat............ NPS Chat
  [ ] omw-1.4............. Open Multilingual Wordnet
  [ ] omw................. Open Multilingual Wordnet
  [ ] opinion_lexicon..... Opinion Lexicon
Hit Enter to continue: q

---------------------------------------------------------------------------
    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit
---------------------------------------------------------------------------
Downloader> q
>messages = [line.rstrip() for line in open('/content/SMSSpamCollection')] # rstrip remove the trailing characters...dataset 
     

>print(len(messages))
     
5574

>messages[0]
     
'ham\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...'

>for mess_no,message in enumerate(messages[:10]):
  print(mess_no,message)
  print('\n')
     
0 ham	Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...


1 ham	Ok lar... Joking wif u oni...


2 spam	Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's
3 ham	U dun say so early hor... U c already then say...


4 ham	Nah I don't think he goes to usf, he lives around here though


5 spam	FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, £1.50 to rcv


6 ham	Even my brother is not like to speak with me. They treat me like aids patent.


7 ham	As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune


8 spam	WINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.


9 spam	Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030

>messages = pd.read_csv('/content/SMSSpamCollection',sep='\t',names = ['label','message'])
     

>messages.head()
     
label	message
0	ham	Go until jurong point, crazy.. Available only ...
1	ham	Ok lar... Joking wif u oni...
2	spam	Free entry in 2 a wkly comp to win FA Cup fina...
3	ham	U dun say so early hor... U c already then say...
4	ham	Nah I don't think he goes to usf, he lives aro...
>messages.message[2]
     
"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's"

#explore BOW
#remove the punctuations and stopwords
     

>import string
     
Sample code


>mess = 'It has many old number of messages and memmories within it'
     

>string.punctuation
     
'!"#$%&\'()*+,-./:;<=>?@[\\]^_`{|}~'

>nopunc = [c for c in mess if c not in string.punctuation]
     

>nopunc
['I',
 't',
 ' ',
 'h',
 'a',
 's',
 ' ',
 'm',
 'a',
 'n',
 'y',
 ' ',
 'o',
 'l',
 'd',
 ' ',
 'n',
 'u',
 'm',
 'b',
 'e',
 'r',
 ' ',
 'o',
 'f',
 ' ',
 'm',
 'e',
 's',
 's',
 'a',
 'g',
 'e',
 's',
 ' ',
 'a',
 'n',
'd',
 ' ',
 'm',
 'e',
 'm',
 'm',
 'o',
 'r',
 'i',
 'e',
 's',
 ' ',
 'w',
 'i',
 't',
 'h',
 'i',
 'n',
 ' ',
 'i',
 't']

>nopunc=''.join(nopunc)
     

>nopunc
     
'It has many old number of messages and memmories within it'

>from nltk.corpus import stopwords
     

>nopunc.split()
['It',
 'has',
 'many',
 'old',
 'number',
 'of',
 'messages',
 'and',
 'memmories',
 'within',
 'it']

>nltk.download("stopwords")
     
[nltk_data] Downloading package stopwords to /root/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
True

>clean_mess = [word for word in nopunc.split() if word.lower() not in stopwords.words("english")]
     

>clean_mess
     
['many', 'old', 'number', 'messages', 'memmories', 'within']

>def preprocess(mess):
  nopunc = [c for c in mess if c not in string.punctuation]
  nopunc=''.join(nopunc)
  clean_mess = [word for word in nopunc.split() if word.lower() not in stopwords.words("english")]
  return clean_mess
     

>messages.message.head(5).apply(preprocess)
0    [Go, jurong, point, crazy, Available, bugis, n...
1                       [Ok, lar, Joking, wif, u, oni]
2    [Free, entry, 2, wkly, comp, win, FA, Cup, fin...
3        [U, dun, say, early, hor, U, c, already, say]
4    [Nah, dont, think, goes, usf, lives, around, t...
Name: message, dtype: object

>from sklearn.feature_extraction.text import CountVectorizer
     

>bow_transformer = CountVectorizer(analyzer=preprocess).fit(messages['message'])
     

>print(len(bow_transformer.vocabulary_))
     
11425

>mess4 = messages["message"][6]
     

>print(mess4)
     
Even my brother is not like to speak with me. They treat me like aids patent.

>bow4 = bow_transformer.transform([mess4])
     

>print(bow4)
(0, 1802)	1
  (0, 4590)	1
  (0, 5193)	1
  (0, 7800)	2
  (0, 8761)	1
  (0, 9971)	1
  (0, 10629)	1

>bow_transformer.get_feature_names()[7800]
     
/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.
  warnings.warn(msg, category=FutureWarning)
'like'

>messages_bow = bow_transformer.transform(messages['message'])
     

>print("Shape of the Sparse Matrix : ",messages_bow.shape)
     
Shape of the Sparse Matrix :  (5572, 11425)

>messages_bow.nnz
     
50548
>from sklearn.feature_extraction.text import TfidfTransformer
     

>tfidf_transformer = TfidfTransformer().fit(messages_bow)
     

>tfidf4 = tfidf_transformer.transform(bow4)
     

>print(tfidf4)
     
  (0, 10629)	0.3352766696931058
  (0, 9971)	0.3268691780062757
  (0, 8761)	0.43700993321905807
  (0, 7800)	0.41453906826037096
  (0, 5193)	0.33843411088434017
  (0, 4590)	0.43700993321905807
  (0, 1802)	0.3352766696931058

>messages_tfidf = tfidf_transformer.transform(messages_bow)
     

>from sklearn import model_selection,svm
     

>clf = svm.SVC()
     

>model = clf.fit(messages_tfidf,messages['label'])
     

>all_pred = model.predict(messages_tfidf)
     

>from sklearn.model_selection import train_test_split
>msg_train,msg_test,label_train,label_test = train_test_split(messages['message'],messages['label'],test_size=.3)
     

>predict = model.predict(messages_tfidf)
     

>from sklearn.metrics import classification_report
>print (classification_report(messages['label'], predict))
     
              precision    recall  f1-score   support

         ham       1.00      1.00      1.00      4825
        spam       1.00      0.99      0.99       747

    accuracy                           1.00      5572
   macro avg       1.00      0.99      1.00      5572
weighted avg       1.00      1.00      1.00      5572

     